{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7e98142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>run_id</th>\n",
       "      <th>feature_step</th>\n",
       "      <th>target_step</th>\n",
       "      <th>target_loss</th>\n",
       "      <th>delta_steps</th>\n",
       "      <th>last_loss</th>\n",
       "      <th>derivative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>a9fa312754c8461e9e6920f6567958f5</td>\n",
       "      <td>1148</td>\n",
       "      <td>3981</td>\n",
       "      <td>0.447833</td>\n",
       "      <td>2833</td>\n",
       "      <td>0.480681</td>\n",
       "      <td>-7.317593e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a9fa312754c8461e9e6920f6567958f5</td>\n",
       "      <td>3467</td>\n",
       "      <td>4300</td>\n",
       "      <td>0.447104</td>\n",
       "      <td>833</td>\n",
       "      <td>0.449832</td>\n",
       "      <td>-9.536021e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a9fa312754c8461e9e6920f6567958f5</td>\n",
       "      <td>831</td>\n",
       "      <td>3981</td>\n",
       "      <td>0.447833</td>\n",
       "      <td>3150</td>\n",
       "      <td>0.499638</td>\n",
       "      <td>-6.644935e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>a9fa312754c8461e9e6920f6567958f5</td>\n",
       "      <td>1288</td>\n",
       "      <td>3311</td>\n",
       "      <td>0.450306</td>\n",
       "      <td>2023</td>\n",
       "      <td>0.476144</td>\n",
       "      <td>-2.659880e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a9fa312754c8461e9e6920f6567958f5</td>\n",
       "      <td>3090</td>\n",
       "      <td>3715</td>\n",
       "      <td>0.448127</td>\n",
       "      <td>625</td>\n",
       "      <td>0.451319</td>\n",
       "      <td>-1.136788e-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1895</th>\n",
       "      <td>4f09218fe6d1488985d184547a6e4c4f</td>\n",
       "      <td>467</td>\n",
       "      <td>3981</td>\n",
       "      <td>0.457133</td>\n",
       "      <td>3514</td>\n",
       "      <td>0.517019</td>\n",
       "      <td>-1.020339e-07</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>4f09218fe6d1488985d184547a6e4c4f</td>\n",
       "      <td>2454</td>\n",
       "      <td>2951</td>\n",
       "      <td>0.462584</td>\n",
       "      <td>497</td>\n",
       "      <td>0.466161</td>\n",
       "      <td>-2.085648e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1897</th>\n",
       "      <td>4f09218fe6d1488985d184547a6e4c4f</td>\n",
       "      <td>724</td>\n",
       "      <td>3981</td>\n",
       "      <td>0.457133</td>\n",
       "      <td>3257</td>\n",
       "      <td>0.503851</td>\n",
       "      <td>-5.008175e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1898</th>\n",
       "      <td>4f09218fe6d1488985d184547a6e4c4f</td>\n",
       "      <td>1258</td>\n",
       "      <td>4168</td>\n",
       "      <td>0.456692</td>\n",
       "      <td>2910</td>\n",
       "      <td>0.489097</td>\n",
       "      <td>-2.907030e-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1899</th>\n",
       "      <td>4f09218fe6d1488985d184547a6e4c4f</td>\n",
       "      <td>616</td>\n",
       "      <td>3890</td>\n",
       "      <td>0.457923</td>\n",
       "      <td>3274</td>\n",
       "      <td>0.511684</td>\n",
       "      <td>-8.932881e-05</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1900 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                run_id  feature_step  target_step  \\\n",
       "0     a9fa312754c8461e9e6920f6567958f5          1148         3981   \n",
       "1     a9fa312754c8461e9e6920f6567958f5          3467         4300   \n",
       "2     a9fa312754c8461e9e6920f6567958f5           831         3981   \n",
       "3     a9fa312754c8461e9e6920f6567958f5          1288         3311   \n",
       "4     a9fa312754c8461e9e6920f6567958f5          3090         3715   \n",
       "...                                ...           ...          ...   \n",
       "1895  4f09218fe6d1488985d184547a6e4c4f           467         3981   \n",
       "1896  4f09218fe6d1488985d184547a6e4c4f          2454         2951   \n",
       "1897  4f09218fe6d1488985d184547a6e4c4f           724         3981   \n",
       "1898  4f09218fe6d1488985d184547a6e4c4f          1258         4168   \n",
       "1899  4f09218fe6d1488985d184547a6e4c4f           616         3890   \n",
       "\n",
       "      target_loss  delta_steps  last_loss    derivative  \n",
       "0        0.447833         2833   0.480681 -7.317593e-06  \n",
       "1        0.447104          833   0.449832 -9.536021e-06  \n",
       "2        0.447833         3150   0.499638 -6.644935e-06  \n",
       "3        0.450306         2023   0.476144 -2.659880e-05  \n",
       "4        0.448127          625   0.451319 -1.136788e-06  \n",
       "...           ...          ...        ...           ...  \n",
       "1895     0.457133         3514   0.517019 -1.020339e-07  \n",
       "1896     0.462584          497   0.466161 -2.085648e-05  \n",
       "1897     0.457133         3257   0.503851 -5.008175e-05  \n",
       "1898     0.456692         2910   0.489097 -2.907030e-05  \n",
       "1899     0.457923         3274   0.511684 -8.932881e-05  \n",
       "\n",
       "[1900 rows x 7 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from lossmoother import LosSmoother\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def _subsample_logspace(length: int, gap: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Subsample steps [1, length] in log10 space with bin width `gap`,\n",
    "    always including the last step.\n",
    "    \"\"\"\n",
    "    pos = np.arange(length)          # 0, 1, ..., length-1\n",
    "    logs = np.log10(pos + 1)         # log10 of steps 1..length\n",
    "    bins = np.floor(logs / gap).astype(int)\n",
    "\n",
    "    # Traverse positions in reverse so we keep the last step per bin\n",
    "    rev_pos = pos[::-1]\n",
    "    _, first_rev_idx = np.unique(bins[rev_pos], return_index=True)\n",
    "\n",
    "    # Map back to original positions, sort, then convert to 1-based steps\n",
    "    chosen_steps = np.sort(rev_pos[first_rev_idx]) + 1\n",
    "    return chosen_steps\n",
    "\n",
    "\n",
    "def _create_datapoints(length, gap: float = 0.01, feature_cutoff: float = 0.6, target_cutoff: float = 0.9, max_datapoints: int = 100):\n",
    "    samples = _subsample_logspace(length, gap)\n",
    "    cutoff = lambda c: int(len(samples) * c)\n",
    "    pairs = [\n",
    "        (feature_step, target_step)\n",
    "        for feature_step in samples[cutoff(feature_cutoff):]\n",
    "        for target_step in samples[cutoff(target_cutoff):]\n",
    "        if feature_step < target_step\n",
    "    ]\n",
    "    if len(pairs) > max_datapoints:\n",
    "        pairs = random.sample(pairs, max_datapoints)\n",
    "    return zip(*pairs)\n",
    "\n",
    "\n",
    "def create_dataset(path: str = 'src/runs_data.json', total: int = 4300, feature_callables: list[callable] = []) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process runs_experiments and create a dataframe with processed loss data.\n",
    "    \"\"\" \n",
    "    with open(path, 'r') as f:\n",
    "        runs_experiments = json.load(f)\n",
    "    \n",
    "    runs_data = {}\n",
    "    for run in runs_experiments:\n",
    "        if len(run['train_loss']) > total:         \n",
    "            feature_steps, target_steps = _create_datapoints(len(run['train_loss'][:total]))\n",
    "            runs_data[run['run_id']] = {\n",
    "                'raw_losses': run['train_loss'][:total],\n",
    "                'feature_steps': feature_steps,\n",
    "                'target_steps': target_steps,\n",
    "                'delta_steps': [t - f for t, f in zip(target_steps, feature_steps)],\n",
    "            }\n",
    "\n",
    "    # Preprocess losses for each run using LosSmoother and collect target losses\n",
    "    for run_data in runs_data.values():\n",
    "        lossmother = LosSmoother()\n",
    "        run_data['preprocessed_losses'] = [lossmother.update(loss)[1] for loss in run_data['raw_losses']]\n",
    "        run_data['target_losses'] = [run_data['preprocessed_losses'][t-1] for t in run_data['target_steps']]\n",
    "\n",
    "    df = pd.DataFrame(\n",
    "        {'run_id': run_id, 'feature_step': f_step, 'target_step': t_step, 'target_loss': t_loss, 'delta_steps': delta_steps}\n",
    "        for run_id, run_data in runs_data.items()\n",
    "        for f_step, t_step, t_loss, delta_steps in zip(run_data['feature_steps'], run_data['target_steps'], run_data['target_losses'], run_data['delta_steps'])\n",
    "    )\n",
    "\n",
    "    for feature_fn in feature_callables:\n",
    "        for idx, row in df.iterrows():\n",
    "            cutoff_loss = runs_data[row['run_id']]['preprocessed_losses'][:row['feature_step']] # step is already + 1, so it will be inclusded\n",
    "            df.loc[idx, feature_fn.__name__] = feature_fn(cutoff_loss)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def last_loss(loss: list) -> float:\n",
    "    return loss[-1]\n",
    "\n",
    "def derivative(loss: list) -> float:\n",
    "    last_loss = loss[-1]\n",
    "    for i in range(len(loss)-1, 0, -1):\n",
    "        if loss[i] != last_loss:\n",
    "            break\n",
    "    else:\n",
    "        return 0\n",
    "    return (loss[i] - last_loss) / (i - len(loss))\n",
    "\n",
    "\n",
    "df = create_dataset(path='runs_data.json', feature_callables=[last_loss, derivative])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfaacfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(561400,\n",
       " 561400,\n",
       " [3997, 3997, 3997, 3997, 3998, 3998, 3998, 3999, 3999, 4000],\n",
       " [2401, 2401, 2401, 2401, 2401, 2401, 2401, 2401, 2401, 2401])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "def _subsample_uniformly_in_logspace(df, col, gap):\n",
    "    \"\"\"\n",
    "    Uniformly subsample rows by uniquely binning `col` with width `gap`.\n",
    "    \"\"\"\n",
    "    key = np.floor(df[col] / gap).astype(int)\n",
    "    return df.loc[key.drop_duplicates().index]\n",
    "\n",
    "\n",
    "def _create_datapoints(length, gap: int, feature_cutoff = 0.4, target_cutoff = 0.1):\n",
    "    assert gap > 0\n",
    "    target_ids = list(range(int(length - length * target_cutoff), length + 1, gap))\n",
    "    feature_ids = list(range(int(length - length * feature_cutoff), length + 1, gap))\n",
    "    feature_steps = [f+1 for f in feature_ids for t in target_ids if f < t]\n",
    "    target_steps = [t+1 for f in feature_ids for t in target_ids if f < t]\n",
    "    return feature_steps, target_steps\n",
    "\n",
    "\n",
    "\n",
    "a, b = _create_datapoints(4000, 1)\n",
    "len(a), len(b), a[-10:], a[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b740c59e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,\n",
       " 100,\n",
       " {np.int64(363),\n",
       "  np.int64(380),\n",
       "  np.int64(398),\n",
       "  np.int64(416),\n",
       "  np.int64(436),\n",
       "  np.int64(457),\n",
       "  np.int64(478),\n",
       "  np.int64(524),\n",
       "  np.int64(549),\n",
       "  np.int64(575),\n",
       "  np.int64(602),\n",
       "  np.int64(630),\n",
       "  np.int64(660),\n",
       "  np.int64(691),\n",
       "  np.int64(724),\n",
       "  np.int64(758),\n",
       "  np.int64(794),\n",
       "  np.int64(831),\n",
       "  np.int64(870),\n",
       "  np.int64(912),\n",
       "  np.int64(954),\n",
       "  np.int64(999),\n",
       "  np.int64(1047),\n",
       "  np.int64(1096),\n",
       "  np.int64(1148),\n",
       "  np.int64(1202),\n",
       "  np.int64(1318),\n",
       "  np.int64(1380),\n",
       "  np.int64(1445),\n",
       "  np.int64(1513),\n",
       "  np.int64(1584),\n",
       "  np.int64(1659),\n",
       "  np.int64(1737),\n",
       "  np.int64(1819),\n",
       "  np.int64(1905),\n",
       "  np.int64(1995),\n",
       "  np.int64(2089),\n",
       "  np.int64(2290),\n",
       "  np.int64(2398),\n",
       "  np.int64(2511),\n",
       "  np.int64(2630),\n",
       "  np.int64(2754),\n",
       "  np.int64(2884),\n",
       "  np.int64(3019),\n",
       "  np.int64(3311),\n",
       "  np.int64(4168)},\n",
       " {np.int64(2398),\n",
       "  np.int64(2511),\n",
       "  np.int64(2630),\n",
       "  np.int64(2754),\n",
       "  np.int64(2884),\n",
       "  np.int64(3019),\n",
       "  np.int64(3162),\n",
       "  np.int64(3311),\n",
       "  np.int64(3467),\n",
       "  np.int64(3630),\n",
       "  np.int64(3801),\n",
       "  np.int64(3981),\n",
       "  np.int64(4168),\n",
       "  np.int64(4300)})"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def _subsample_logspace(length: int, gap: float) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Subsample steps [1, length] in log10 space with bin width `gap`,\n",
    "    always including the last step.\n",
    "    \"\"\"\n",
    "    pos = np.arange(length)          # 0, 1, ..., length-1\n",
    "    logs = np.log10(pos + 1)         # log10 of steps 1..length\n",
    "    bins = np.floor(logs / gap).astype(int)\n",
    "\n",
    "    # Traverse positions in reverse so we keep the last step per bin\n",
    "    rev_pos = pos[::-1]\n",
    "    _, first_rev_idx = np.unique(bins[rev_pos], return_index=True)\n",
    "\n",
    "    # Map back to original positions, sort, then convert to 1-based steps\n",
    "    chosen_steps = np.sort(rev_pos[first_rev_idx]) + 1\n",
    "    return chosen_steps\n",
    "\n",
    "\n",
    "def _create_datapoints(length, gap: float = 0.02, feature_cutoff: float = 0.6, target_cutoff: float = 0.9, max_datapoints: int = 100):\n",
    "    samples = _subsample_logspace(length, gap)\n",
    "    cutoff = lambda c: int(len(samples) * c)\n",
    "    pairs = [\n",
    "        (feature_step, target_step)\n",
    "        for feature_step in samples[cutoff(feature_cutoff):]\n",
    "        for target_step in samples[cutoff(target_cutoff):]\n",
    "        if feature_step < target_step\n",
    "    ]\n",
    "    if len(pairs) > max_datapoints:\n",
    "        pairs = random.sample(pairs, max_datapoints)\n",
    "    return zip(*pairs)\n",
    "\n",
    "\n",
    "fs, ts = _create_datapoints(4300, 0.02)\n",
    "\n",
    "len(fs), len(ts), set(fs), set(ts)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
